{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis - Iris Dataset\n\nThis notebook provides comprehensive statistical analysis including:\n- Descriptive Statistics\n- Inferential Statistics\n- Exploratory Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import ttest_ind, f_oneway, normaltest, shapiro, levene, kruskal\nfrom itertools import combinations\nimport warnings\nfrom pathlib import Path\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 8)\nnp.random.seed(42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_path = Path('../../data/Iris.csv')\ndf = pd.read_csv(data_path)\nfeatures = ['sepal.length', 'sepal.width', 'petal.length', 'petal.width']\nprint('Data loaded successfully!')\nprint(f'Dataset shape: {df.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('=' * 80)\nprint('DESCRIPTIVE STATISTICS')\nprint('=' * 80)\nprint(df[features].describe())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('\\n' + '=' * 80)\nprint('DESCRIPTIVE STATISTICS BY VARIETY')\nprint('=' * 80)\nfor variety in df['variety'].unique():\n    print(f'\\n{variety}:')\n    print(df[df['variety'] == variety][features].describe())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "descriptive_stats = pd.DataFrame({\n    'Mean': df[features].mean(),\n    'Median': df[features].median(),\n    'Std Dev': df[features].std(),\n    'Variance': df[features].var(),\n    'Skewness': df[features].skew(),\n    'Kurtosis': df[features].kurtosis(),\n    'Min': df[features].min(),\n    'Max': df[features].max(),\n    'Range': df[features].max() - df[features].min(),\n    'IQR': df[features].quantile(0.75) - df[features].quantile(0.25),\n    'CV': (df[features].std() / df[features].mean()) * 100\n})\nprint(descriptive_stats.round(3))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normality Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('=' * 80)\nprint('NORMALITY TESTS (Shapiro-Wilk Test)')\nprint('=' * 80)\n\nnormality_results = []\nfor feature in features:\n    for variety in df['variety'].unique():\n        data = df[df['variety'] == variety][feature]\n        stat, p_value = shapiro(data)\n        normality_results.append({\n            'Feature': feature,\n            'Variety': variety,\n            'Statistic': stat,\n            'P-value': p_value,\n            'Normal': 'Yes' if p_value > 0.05 else 'No'\n        })\n\nnormality_df = pd.DataFrame(normality_results)\nprint(normality_df.round(4))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inferential Statistics - ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('=' * 80)\nprint('ONE-WAY ANOVA TEST')\nprint('=' * 80)\nprint('H0: All groups have the same mean')\nprint('H1: At least one group has a different mean\\n')\n\nanova_results = []\nfor feature in features:\n    groups = [df[df['variety'] == variety][feature].values for variety in df['variety'].unique()]\n    f_stat, p_value = f_oneway(*groups)\n    anova_results.append({\n        'Feature': feature,\n        'F-statistic': f_stat,\n        'P-value': p_value,\n        'Significant': 'Yes' if p_value < 0.05 else 'No'\n    })\n\nanova_df = pd.DataFrame(anova_results)\nprint(anova_df.round(6))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pairwise t-tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "varieties = df['variety'].unique()\npairs = list(combinations(varieties, 2))\n\nprint('=' * 80)\nprint('PAIRWISE T-TESTS')\nprint('=' * 80)\n\nttest_results = []\nfor feature in features:\n    for pair in pairs:\n        group1 = df[df['variety'] == pair[0]][feature]\n        group2 = df[df['variety'] == pair[1]][feature]\n        _, levene_p = levene(group1, group2)\n        equal_var = levene_p > 0.05\n        t_stat, p_value = ttest_ind(group1, group2, equal_var=equal_var)\n        ttest_results.append({\n            'Feature': feature,\n            'Group1': pair[0],\n            'Group2': pair[1],\n            'T-statistic': t_stat,\n            'P-value': p_value,\n            'Significant': 'Yes' if p_value < 0.05 else 'No'\n        })\n\nttest_df = pd.DataFrame(ttest_results)\nprint(ttest_df.round(6))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('=' * 80)\nprint('PEARSON CORRELATION COEFFICIENTS')\nprint('=' * 80)\ncorr_matrix = df[features].corr()\nprint(corr_matrix.round(4))\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, \n            linewidths=1, cbar_kws={'shrink': 0.8})\nplt.title('Pearson Correlation Matrix', fontsize=16)\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def confidence_interval(data, confidence=0.95):\n    n = len(data)\n    mean = np.mean(data)\n    std_err = stats.sem(data)\n    h = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n    return mean - h, mean + h\n\nprint('=' * 80)\nprint('95% CONFIDENCE INTERVALS FOR MEANS')\nprint('=' * 80)\n\nci_results = []\nfor feature in features:\n    for variety in df['variety'].unique():\n        data = df[df['variety'] == variety][feature]\n        ci_lower, ci_upper = confidence_interval(data)\n        mean_val = data.mean()\n        ci_results.append({\n            'Feature': feature,\n            'Variety': variety,\n            'Mean': mean_val,\n            'CI Lower': ci_lower,\n            'CI Upper': ci_upper\n        })\n\nci_df = pd.DataFrame(ci_results)\nprint(ci_df.round(4))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}